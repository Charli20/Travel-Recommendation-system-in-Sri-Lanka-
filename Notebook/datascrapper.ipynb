{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08e9c9b1",
   "metadata": {},
   "source": [
    "* Check the python version by {python --version} make sure its higher or equal to 3.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c2609d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install selenium beautifulsoup4 pandas webdriver-manager on terminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0b1d9036",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import math\n",
    "import re\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af4670a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- SETUP ----------------\n",
    "options = Options()\n",
    "options.add_argument(\"--start-maximized\")\n",
    "options.add_argument(\"--disable-blink-features=AutomationControlled\")\n",
    "\n",
    "driver = webdriver.Chrome(options=options) # If this does not work, uncomment the below code and comment this line and comment this line \n",
    "# from selenium.webdriver.chrome.service import Service\n",
    "# from webdriver_manager.chrome import ChromeDriverManager\n",
    "\n",
    "# driver = webdriver.Chrome(\n",
    "#     service=Service(ChromeDriverManager().install()),\n",
    "#     options=options\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "92617ae7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------- LIST OF URLS ----------------\n",
    "URLS = [ # Add your URLs here\n",
    "    \"https://www.tripadvisor.com/Attraction_Review-g1407334-d6104104-Reviews-Mirissa_Beach-Mirissa_Southern_Province.html\",\n",
    "    \"https://www.tripadvisor.com/Attraction_Review-g304134-d7034305-Reviews-Hikkaduwa_Beach-Hikkaduwa_Galle_District_Southern_Province.html\",\n",
    "    \"https://www.tripadvisor.com/Attraction_Review-g644047-d3331573-Reviews-Jungle_Beach-Unawatuna_Galle_District_Southern_Province.html\",\n",
    "    \"https://www.tripadvisor.com/Attraction_Review-g297897-d1655656-Reviews-Negombo_Beach-Negombo_Western_Province.html\",\n",
    "    \"https://www.tripadvisor.com/Attraction_Review-g293962-d554029-Reviews-Mount_Lavinia_Beach-Colombo_Western_Province.html\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a098b2f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing destination: https://www.tripadvisor.com/Attraction_Review-g1407334-d6104104-Reviews-Mirissa_Beach-Mirissa_Southern_Province.html\n",
      "Destination: Mirissa Beach | Reviews to scrape: 500\n",
      "  Scraping page 1/50\n"
     ]
    },
    {
     "ename": "InvalidSessionIdException",
     "evalue": "Message: invalid session id: session deleted as the browser has closed the connection\nfrom disconnected: not connected to DevTools\n  (Session info: chrome=143.0.7499.40); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalidsessionidexception\nStacktrace:\nSymbols not available. Dumping unresolved backtrace:\n\t0x7ff681eb88e5\n\t0x7ff681eb8940\n\t0x7ff681c9165d\n\t0x7ff681c7d202\n\t0x7ff681ca27af\n\t0x7ff681d19a29\n\t0x7ff681d3a5c2\n\t0x7ff681cdac29\n\t0x7ff681cdba93\n\t0x7ff6821d0640\n\t0x7ff6821caf80\n\t0x7ff6821e96e6\n\t0x7ff681ed5de4\n\t0x7ff681eded8c\n\t0x7ff681ec2004\n\t0x7ff681ec21b5\n\t0x7ff681ea7ee2\n\t0x7ffbf303e8d7\n\t0x7ffbf4e0c53c\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidSessionIdException\u001b[39m                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 56\u001b[39m\n\u001b[32m     53\u001b[39m     page_url = full_url.replace(\u001b[33m\"\u001b[39m\u001b[33mReviews-\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReviews-or\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;250m \u001b[39m*\u001b[38;5;250m \u001b[39mreviews_per_page\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m-\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     55\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Scraping page \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpage\u001b[38;5;250m \u001b[39m+\u001b[38;5;250m \u001b[39m\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtotal_pages\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mdriver\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpage_url\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m time.sleep(\u001b[32m5\u001b[39m)\n\u001b[32m     59\u001b[39m soup = BeautifulSoup(driver.page_source, \u001b[33m\"\u001b[39m\u001b[33mhtml.parser\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:452\u001b[39m, in \u001b[36mWebDriver.get\u001b[39m\u001b[34m(self, url)\u001b[39m\n\u001b[32m    439\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mget\u001b[39m(\u001b[38;5;28mself\u001b[39m, url: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    440\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Navigate the browser to the specified URL.\u001b[39;00m\n\u001b[32m    441\u001b[39m \n\u001b[32m    442\u001b[39m \u001b[33;03m    The method does not return until the page is fully loaded (i.e. the\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    450\u001b[39m \u001b[33;03m        `driver.get(\"https://example.com\")`\u001b[39;00m\n\u001b[32m    451\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m452\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mCommand\u001b[49m\u001b[43m.\u001b[49m\u001b[43mGET\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43murl\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43murl\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\webdriver.py:432\u001b[39m, in \u001b[36mWebDriver.execute\u001b[39m\u001b[34m(self, driver_command, params)\u001b[39m\n\u001b[32m    429\u001b[39m response = cast(RemoteConnection, \u001b[38;5;28mself\u001b[39m.command_executor).execute(driver_command, params)\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m response:\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43merror_handler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcheck_response\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresponse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m     response[\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[38;5;28mself\u001b[39m._unwrap_value(response.get(\u001b[33m\"\u001b[39m\u001b[33mvalue\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[32m    434\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m response\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\chali\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\selenium\\webdriver\\remote\\errorhandler.py:232\u001b[39m, in \u001b[36mErrorHandler.check_response\u001b[39m\u001b[34m(self, response)\u001b[39m\n\u001b[32m    230\u001b[39m         alert_text = value[\u001b[33m\"\u001b[39m\u001b[33malert\u001b[39m\u001b[33m\"\u001b[39m].get(\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    231\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace, alert_text)  \u001b[38;5;66;03m# type: ignore[call-arg]  # mypy is not smart enough here\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m232\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception_class(message, screen, stacktrace)\n",
      "\u001b[31mInvalidSessionIdException\u001b[39m: Message: invalid session id: session deleted as the browser has closed the connection\nfrom disconnected: not connected to DevTools\n  (Session info: chrome=143.0.7499.40); For documentation on this error, please visit: https://www.selenium.dev/documentation/webdriver/troubleshooting/errors#invalidsessionidexception\nStacktrace:\nSymbols not available. Dumping unresolved backtrace:\n\t0x7ff681eb88e5\n\t0x7ff681eb8940\n\t0x7ff681c9165d\n\t0x7ff681c7d202\n\t0x7ff681ca27af\n\t0x7ff681d19a29\n\t0x7ff681d3a5c2\n\t0x7ff681cdac29\n\t0x7ff681cdba93\n\t0x7ff6821d0640\n\t0x7ff6821caf80\n\t0x7ff6821e96e6\n\t0x7ff681ed5de4\n\t0x7ff681eded8c\n\t0x7ff681ec2004\n\t0x7ff681ec21b5\n\t0x7ff681ea7ee2\n\t0x7ffbf303e8d7\n\t0x7ffbf4e0c53c\n"
     ]
    }
   ],
   "source": [
    "MAX_REVIEWS_PER_DESTINATION = 500 # Manjitha should change this value for 500\n",
    "\n",
    "# ---------------- OUTPUT FOLDER ----------------\n",
    "output_folder = r\"C:\\NIBM\\Travel recommendation system\\Travel-Recommendation-system-in-Sri-Lanka-\\scraped_data\" #your file path\n",
    "os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "# ---------------- SCRAPING ----------------\n",
    "for full_url in URLS:\n",
    "    print(f\"\\nProcessing destination: {full_url}\")\n",
    "    driver.get(full_url)\n",
    "    time.sleep(5)\n",
    "\n",
    "    soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "\n",
    "  # ---------------- DESTINATION TITLE ----------------\n",
    "    title_tag = soup.select_one(\"h1[data-test-target='mainH1']\")\n",
    "    destination_title = title_tag.get_text(strip=True) if title_tag else \"Unknown\"\n",
    "\n",
    "    safe_title = re.sub(r'[\\\\/*?:\"<>|]', \"\", destination_title)\n",
    "    filename = os.path.join(output_folder, f\"{safe_title}.csv\")\n",
    "\n",
    "    # ---------------- SKIP IF FILE EXISTS ----------------\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"CSV already exists for {destination_title}, skipping...\")\n",
    "        continue\n",
    "\n",
    "    # ---------------- DESTINATION SCORE ----------------\n",
    "    score_tag = soup.select_one(\"div[data-automation='bubbleRatingValue'] span\")\n",
    "    destination_score = score_tag.get_text(strip=True) if score_tag else None\n",
    "\n",
    "    # ---------------- TOTAL REVIEWS ----------------\n",
    "    total_tag = soup.select_one(\"div[data-automation='bubbleReviewCount'] span\")\n",
    "    total_reviews = 0\n",
    "    if total_tag:\n",
    "        total_reviews = int(re.sub(r\"[^\\d]\", \"\", total_tag.get_text()))\n",
    "    total_reviews = min(total_reviews, MAX_REVIEWS_PER_DESTINATION)\n",
    "\n",
    "    # ---------------- DESTINATION IMAGE ----------------\n",
    "    img_tag = soup.select_one(\"img[srcset]\")\n",
    "    destination_image = img_tag[\"src\"] if img_tag else None\n",
    "\n",
    "    print(f\"Destination: {destination_title} | Reviews to scrape: {total_reviews}\")\n",
    "\n",
    "    # ---------------- REVIEW LEVEL ----------------\n",
    "    reviews_per_page = 10\n",
    "    total_pages = math.ceil(total_reviews / reviews_per_page)\n",
    "    all_reviews = []\n",
    "\n",
    "    for page in range(total_pages):\n",
    "        if page == 0:\n",
    "            page_url = full_url\n",
    "        else:\n",
    "            page_url = full_url.replace(\"Reviews-\", f\"Reviews-or{page * reviews_per_page}-\")\n",
    "\n",
    "        print(f\"  Scraping page {page + 1}/{total_pages}\")\n",
    "        driver.get(page_url)\n",
    "        time.sleep(5)\n",
    "\n",
    "        soup = BeautifulSoup(driver.page_source, \"html.parser\")\n",
    "        review_cards = soup.select(\"div[data-automation='reviewCard']\")\n",
    "\n",
    "        for card in review_cards:\n",
    "\n",
    "            # Reviewer name\n",
    "            user_tag = card.select_one(\"span.biGQs._P.ezezH a\")\n",
    "            user = user_tag.get_text(strip=True) if user_tag else None\n",
    "\n",
    "            # Reviewer location\n",
    "            location_tag = card.select_one(\"div.biGQs._P.navcl span\")\n",
    "            location = location_tag.get_text(strip=True) if location_tag else None\n",
    "\n",
    "            # Review title\n",
    "            title_tag = card.select_one(\"h3 span.yCeTE\")\n",
    "            review_title = title_tag.get_text(strip=True) if title_tag else None\n",
    "\n",
    "            # Review text\n",
    "            review_tag = card.select_one(\"div.biGQs._P.VImYz.AWdfh span.yCeTE\")\n",
    "            review_text = review_tag.get_text(\" \", strip=True) if review_tag else None\n",
    "\n",
    "            # Written date\n",
    "            written_tag = card.select_one(\"div.biGQs._P.VImYz.ncFvv.navcl\")\n",
    "            written_date = (\n",
    "                written_tag.get_text(strip=True).replace(\"Written \", \"\")\n",
    "                if written_tag else None\n",
    "            )\n",
    "\n",
    "            # Review score\n",
    "            review_score = None\n",
    "            score_svg = card.select_one(\"svg[data-automation='bubbleRatingImage'] title\")\n",
    "            if score_svg:\n",
    "                match = re.search(r\"(\\d) of 5\", score_svg.get_text())\n",
    "                if match:\n",
    "                    review_score = int(match.group(1))\n",
    "\n",
    "            # Reviewer category (ROBUST METHOD)\n",
    "            category = None\n",
    "            card_text = card.get_text(\" \", strip=True)\n",
    "            cat_match = re.search(\n",
    "                r\"\\b(Couples|Family|Friends|Solo|Business)\\b\",\n",
    "                card_text\n",
    "            )\n",
    "            if cat_match:\n",
    "                category = cat_match.group(1)\n",
    "\n",
    "            all_reviews.append({\n",
    "                \"Destination URL\": full_url,\n",
    "                \"Destination Title\": destination_title,\n",
    "                \"Destination Score\": destination_score,\n",
    "                \"Total Reviews (Destination)\": total_reviews,\n",
    "                \"Destination Image\": destination_image,\n",
    "                \"Reviewer Name\": user,\n",
    "                \"Reviewer Location\": location,\n",
    "                \"Review Title\": review_title,\n",
    "                \"Review Score\": review_score,\n",
    "                \"Reviewer Category\": category,\n",
    "                \"Review Text\": review_text,\n",
    "                \"Written Date\": written_date\n",
    "            })\n",
    "\n",
    "    # ---------------- SAVE CSV ----------------\n",
    "    df = pd.DataFrame(all_reviews)\n",
    "    df.to_csv(filename, index=False, encoding=\"utf-8-sig\")\n",
    "    print(f\"Saved {len(df)} reviews → {filename}\")\n",
    "\n",
    "driver.quit()\n",
    "print(\"\\n✅ Scraping completed successfully.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
